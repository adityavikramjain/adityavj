<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding LLMs: Token by Token | Aditya V Jain</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Space+Grotesk:wght@500;700;800&display=swap" rel="stylesheet">

    <style>
        /* === VARIABLES === */
        :root {
            --electric-orange: #FF4500;
            --electric-orange-light: rgba(255, 69, 0, 0.1);
            --electric-orange-glow: rgba(255, 69, 0, 0.3);
            --white: #FFFFFF;
            --mist-rose: #FFF5F2;
            --charcoal: #1F2937;
            --soft-grey: #6B7280;
            --light-grey: #9CA3AF;
            --border-light: rgba(0, 0, 0, 0.06);
            --ease-smooth: cubic-bezier(0.4, 0, 0.2, 1);
            --shadow-sm: 0 2px 8px rgba(0, 0, 0, 0.04);
            --shadow-md: 0 4px 20px rgba(0, 0, 0, 0.08);
        }

        /* === GLOBAL === */
        * { box-sizing: border-box; margin: 0; padding: 0; }

        html { scroll-behavior: smooth; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            color: var(--charcoal);
            background: var(--mist-rose);
            line-height: 1.75;
            -webkit-font-smoothing: antialiased;
        }

        /* === CONTAINER === */
        .blog-container {
            background:
                radial-gradient(ellipse 80% 50% at 20% 80%, rgba(255, 69, 0, 0.08) 0%, transparent 50%),
                radial-gradient(ellipse 60% 40% at 85% 20%, rgba(255, 69, 0, 0.06) 0%, transparent 50%),
                var(--mist-rose);
            min-height: 100vh;
            padding: 60px 40px;
        }

        /* === TERMINAL HEADER === */
        .terminal-header {
            font-family: 'Space Grotesk', monospace;
            font-weight: 700;
            font-size: 1.5rem;
            color: var(--charcoal);
            margin-bottom: 40px;
            display: flex;
            align-items: center;
            opacity: 0;
            animation: slideIn 0.6s var(--ease-smooth) forwards;
        }

        .terminal-text { letter-spacing: -0.02em; }
        .terminal-prompt { color: var(--electric-orange); }

        .back-link {
            margin-left: auto;
            font-size: 0.9rem;
            color: var(--electric-orange);
            text-decoration: none;
            font-weight: 600;
            transition: all 0.2s ease;
            padding: 8px 16px;
            border-radius: 8px;
            background: var(--white);
            border: 1px solid var(--border-light);
        }

        .back-link:hover {
            background: var(--electric-orange-light);
            transform: translateX(-4px);
        }

        /* === ARTICLE === */
        .article-wrapper {
            max-width: 800px;
            margin: 0 auto;
            background: var(--white);
            border-radius: 24px;
            padding: 80px;
            box-shadow: var(--shadow-md);
            border: 1px solid var(--border-light);
            opacity: 0;
            animation: fadeInUp 0.8s var(--ease-smooth) 0.2s forwards;
        }

        .article-meta {
            display: flex;
            align-items: center;
            gap: 16px;
            margin-bottom: 32px;
            font-size: 0.9rem;
            color: var(--soft-grey);
        }

        .article-badge {
            font-family: 'Space Grotesk', sans-serif;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.04em;
            padding: 6px 12px;
            border-radius: 6px;
            background: var(--electric-orange-light);
            color: var(--electric-orange);
        }

        .article-date {
            font-weight: 500;
        }

        .article-title {
            font-family: 'Space Grotesk', sans-serif;
            font-size: 3rem;
            font-weight: 800;
            line-height: 1.2;
            letter-spacing: -0.03em;
            color: var(--charcoal);
            margin-bottom: 24px;
            background: linear-gradient(135deg, var(--charcoal) 0%, #374151 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .article-subtitle {
            font-size: 1.25rem;
            line-height: 1.6;
            color: var(--soft-grey);
            margin-bottom: 48px;
            padding-left: 20px;
            border-left: 3px solid var(--electric-orange);
        }

        /* === TYPOGRAPHY === */
        .article-content h2 {
            font-family: 'Space Grotesk', sans-serif;
            font-size: 2rem;
            font-weight: 700;
            color: var(--charcoal);
            margin-top: 56px;
            margin-bottom: 24px;
            line-height: 1.3;
            letter-spacing: -0.02em;
        }

        .article-content h3 {
            font-family: 'Space Grotesk', sans-serif;
            font-size: 1.5rem;
            font-weight: 700;
            color: var(--charcoal);
            margin-top: 40px;
            margin-bottom: 16px;
            line-height: 1.4;
        }

        .article-content h4 {
            font-family: 'Space Grotesk', sans-serif;
            font-size: 1.2rem;
            font-weight: 600;
            color: var(--charcoal);
            margin-top: 32px;
            margin-bottom: 12px;
        }

        .article-content p {
            font-size: 1.1rem;
            line-height: 1.75;
            color: var(--charcoal);
            margin-bottom: 24px;
        }

        .article-content strong {
            font-weight: 700;
            color: var(--charcoal);
        }

        .article-content a {
            color: var(--electric-orange);
            text-decoration: none;
            font-weight: 600;
            border-bottom: 2px solid var(--electric-orange-light);
            transition: all 0.2s ease;
        }

        .article-content a:hover {
            border-bottom-color: var(--electric-orange);
            background: var(--electric-orange-light);
        }

        .article-content ul,
        .article-content ol {
            margin-bottom: 24px;
            padding-left: 32px;
        }

        .article-content li {
            font-size: 1.1rem;
            line-height: 1.75;
            color: var(--charcoal);
            margin-bottom: 12px;
        }

        .article-content hr {
            border: none;
            height: 1px;
            background: var(--border-light);
            margin: 56px 0;
        }

        .article-content blockquote {
            margin: 32px 0;
            padding: 24px 32px;
            background: var(--electric-orange-light);
            border-left: 4px solid var(--electric-orange);
            border-radius: 8px;
            font-style: italic;
            color: var(--soft-grey);
        }

        .article-content code {
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            background: var(--mist-rose);
            padding: 3px 8px;
            border-radius: 4px;
            color: var(--electric-orange);
        }

        /* === HIGHLIGHT BOX === */
        .highlight-box {
            background: linear-gradient(135deg, var(--white) 0%, rgba(255, 69, 0, 0.03) 100%);
            border: 2px solid var(--electric-orange-light);
            border-radius: 16px;
            padding: 32px;
            margin: 40px 0;
        }

        .highlight-box h3 {
            margin-top: 0 !important;
            color: var(--electric-orange);
        }

        /* === CTA SECTION === */
        .cta-section {
            background: var(--electric-orange);
            color: var(--white);
            padding: 40px;
            border-radius: 16px;
            text-align: center;
            margin-top: 56px;
        }

        .cta-section p {
            color: var(--white);
            font-size: 1.2rem;
            margin-bottom: 24px;
        }

        .cta-button {
            display: inline-block;
            background: var(--white);
            color: var(--electric-orange);
            padding: 16px 32px;
            border-radius: 10px;
            text-decoration: none;
            font-weight: 700;
            font-size: 1.05rem;
            transition: all 0.3s var(--ease-smooth);
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
        }

        .cta-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.25);
        }

        /* === FOOTER === */
        .article-footer {
            max-width: 800px;
            margin: 40px auto 0;
            padding-top: 32px;
            border-top: 1px solid var(--border-light);
            text-align: center;
        }

        .article-footer p {
            color: var(--soft-grey);
            font-size: 0.95rem;
        }

        .article-footer a {
            color: var(--electric-orange);
            text-decoration: none;
            font-weight: 600;
        }

        .article-footer a:hover {
            text-decoration: underline;
        }

        /* === ANIMATIONS === */
        @keyframes slideIn {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        @keyframes fadeInUp {
            from { opacity: 0; transform: translateY(30px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* === RESPONSIVE === */
        @media (max-width: 768px) {
            .blog-container {
                padding: 40px 20px;
            }

            .terminal-header {
                flex-direction: column;
                align-items: flex-start;
                gap: 16px;
            }

            .back-link {
                margin-left: 0;
            }

            .article-wrapper {
                padding: 40px 32px;
            }

            .article-title {
                font-size: 2rem;
            }

            .article-content h2 {
                font-size: 1.5rem;
            }

            .article-content h3 {
                font-size: 1.25rem;
            }

            .article-content p,
            .article-content li {
                font-size: 1rem;
            }
        }

        @media (max-width: 480px) {
            .article-wrapper {
                padding: 32px 24px;
                border-radius: 16px;
            }

            .article-title {
                font-size: 1.75rem;
            }
        }
    </style>
</head>
<body>

<div class="blog-container">
    <!-- Terminal Header -->
    <div class="terminal-header">
        <span class="terminal-text"><span class="terminal-prompt">&gt;</span> adityavj.com/blog</span>
        <a href="index.html" class="back-link">← Back to Home</a>
    </div>

    <!-- Article -->
    <article class="article-wrapper">
        <div class="article-meta">
            <span class="article-badge">Deep Dive</span>
            <span class="article-date">January 2026</span>
        </div>

        <h1 class="article-title">Understanding How LLMs Think: Token by Token</h1>

        <div class="article-subtitle">
            Business leaders throw around terms like "hallucination" and "temperature" without understanding the mechanics. Here's what's actually happening inside these models—and why it matters for your AI strategy.
        </div>

        <div class="article-content">
            <p>Every executive I speak with uses AI. They write prompts, they get responses, they're impressed by the magic. But when I ask them to explain <em>why</em> ChatGPT sometimes makes things up, or what "temperature" actually controls, I get hand-waving.</p>

            <p>This isn't academic curiosity. Understanding the mechanics changes how you prompt, how you evaluate outputs, and critically—how you explain AI limitations to your board when someone asks, <em>"Why can't we just trust the AI to write our earnings report?"</em></p>

            <p>So I built an interactive visualization that shows you <strong>exactly</strong> what's happening inside a Large Language Model as it generates text, one token at a time.</p>

            <h2>The Core Mechanic: Auto-Regression</h2>

            <p>Large Language Models are <strong>auto-regressive</strong>. This is not a buzzword—it's a precise technical term that explains their fundamental behavior.</p>

            <p>Auto-regression means: <em>the next output depends on all previous outputs.</em></p>

            <p>When you ask Gemini or ChatGPT a question, it doesn't "think" and then write an answer. It predicts <strong>one token</strong> (roughly a word or word fragment), adds it to the context, then predicts the next token based on everything it's written so far.</p>

            <p>Imagine you're typing an email, but your keyboard only suggests one word at a time. You pick the most likely next word, it appears, and then you get new suggestions based on what you've written. That's how these models work—except they're doing this thousands of times per response, and they're considering probability distributions across millions of possible next tokens.</p>

            <div class="highlight-box">
                <h3>Why This Matters for Business</h3>
                
                <p>This token-by-token generation is why LLMs sometimes "drift" into nonsense. If the model picks a low-probability word early in a response, every subsequent prediction is now conditioned on that error. It's not "thinking ahead" to course-correct—it's committed to the path it started down.</p>

                <p>This is also why <strong>prompt engineering works</strong>. By carefully structuring your input, you're steering the probability distribution toward better first tokens, which then cascade into better subsequent predictions.</p>
            </div>

            <h2>What the Playground Shows You</h2>

            <p>I built the Auto-Regressive Playground to make these mechanics visible. Here's what happens when you click "Step":</p>

            <ol>
                <li><strong>The Model Evaluates:</strong> It takes everything in the context window and computes probability scores for thousands of possible next tokens.</li>
                <li><strong>You See the Top 5:</strong> The visualization shows a bar chart of the five most likely candidates. Maybe "the" has a 45% probability, "a" has 20%, "of" has 15%, and so on.</li>
                <li><strong>The Model Samples:</strong> Based on your temperature setting, it picks one token. At temperature 0 (deterministic), it always picks the highest probability. At temperature 1 (creative), it samples from the distribution—sometimes picking less obvious choices.</li>
                <li><strong>The Token Flies Back:</strong> You see the chosen token animate from the probability chart back into the context window. It's now part of the prompt for the next prediction.</li>
                <li><strong>Repeat:</strong> The cycle starts again with the expanded context.</li>
            </ol>

            <p>By clicking "Step" repeatedly, you watch the model build a response token by token. You see <em>exactly</em> which alternatives it considered and how confident it was in each choice.</p>

            <h2>Temperature: The Creativity Dial</h2>

            <p>Temperature is not "how creative the AI is." That's the marketing speak.</p>

            <p>Technically, temperature controls the <strong>sharpness of the probability distribution</strong>.</p>

            <ul>
                <li><strong>Temperature = 0 (Deterministic):</strong> The model always picks the highest-probability token. Responses are consistent and predictable. Use this for code generation, data extraction, or any task where you need reproducible outputs.</li>
                <li><strong>Temperature = 1 (Creative):</strong> The probability distribution is "flattened." Lower-probability tokens get a fair chance. The model takes more risks. Use this for brainstorming, creative writing, or when you want novel solutions.</li>
            </ul>

            <p>In the Playground, try generating the same prompt at temperature 0 and temperature 1. At 0, you'll see the same token chosen every time. At 1, you'll see variety—sometimes the model picks a less obvious word, leading to a completely different response trajectory.</p>

            <div class="highlight-box">
                <h3>Real-World Application: When to Tune Temperature</h3>

                <p>I've seen companies use temperature 1 for everything because they think "more creative = better." That's wrong.</p>

                <p>When you're using AI to <strong>extract structured data</strong> from documents (like parsing invoices), you want temperature 0. Creativity is a bug, not a feature. You need the model to output "Invoice Number: 12345" every single time, not "Invoice Code: 12345" on one run and "Bill ID: 12345" on another.</p>

                <p>When you're using AI to <strong>generate marketing copy</strong> or brainstorm product names, temperature 0.7-1.0 makes sense. You want unexpected combinations and fresh angles.</p>
            </div>

            <h2>Hallucination Detection: The Low-Probability Warning</h2>

            <p>The Playground includes a "hallucination detector." Here's how it works:</p>

            <p>If the model selects a token with less than 40% probability, a warning overlay appears: <em>"Low Probability Detected. High Hallucination Risk."</em></p>

            <p>Why 40%? Because when a model is <em>uncertain</em> but forced to produce output anyway, it often generates plausible-sounding nonsense. The token it picked wasn't a confident choice—it was the best of several bad options.</p>

            <p>Try the "Test Fake Knowledge" button in the Playground. It asks: <em>"Who was the first Martian President in 1600?"</em></p>

            <p>Watch what happens. The model has never seen this information because it doesn't exist. But it can't say "I don't know"—it's auto-regressive, it <em>must</em> predict the next token. So it picks something that sounds plausible based on the word patterns it's learned.</p>

            <p>You'll see the probability scores. None of the top candidates are confident. The model is guessing. That's a hallucination in progress.</p>

            <div class="highlight-box">
                <h3>Why This Matters for Procurement Teams</h3>

                <p>If you're buying an AI-powered contract analysis tool, and the vendor says it "automatically flags risky clauses," you need to ask: <em>"What happens when the model encounters a clause structure it's never seen before?"</em></p>

                <p>Does it confidently hallucinate a risk assessment? Or does it admit uncertainty?</p>

                <p>Most enterprise AI tools don't expose these probability scores to end users. But now you know what to ask for. Request access to <strong>logprobs</strong> (log probabilities) or confidence scores. If the vendor can't provide them, their system is a black box—and you're flying blind.</p>
            </div>

            <h2>Why I Built This as a Teaching Tool</h2>

            <p>I use this Playground in my IIM Kozhikode sessions because business leaders need to develop <strong>AI intuition</strong>, not just memorize definitions.</p>

            <p>When you watch tokens flow from probability chart to context window, you internalize the loop. You stop thinking of AI as "intelligent" and start seeing it as a <strong>statistical prediction engine</strong>. That shift in mental model is everything.</p>

            <p>It changes questions like:</p>

            <ul>
                <li><em>"Why did the AI give me a different answer this time?"</em> → <em>"What changed in the probability distribution between runs? Did I adjust temperature? Did I reword the prompt, shifting the early token probabilities?"</em></li>
                <li><em>"Can we trust this AI for legal contract review?"</em> → <em>"Can we architect the system to force low-temperature, high-confidence outputs? Can we detect when it's operating in uncertain territory?"</em></li>
            </ul>

            <p>You move from passive acceptance to active engineering.</p>

            <h2>How to Use the Playground</h2>

            <ol>
                <li><strong>Get a Gemini API Key:</strong> The tool uses Google's Gemini API. You'll need a free API key from <a href="https://aistudio.google.com/app/apikey" target="_blank">Google AI Studio</a>.</li>
                <li><strong>Enter a Prompt:</strong> Type any starting text. Try something simple like "The future of AI in India" or use the hallucination test.</li>
                <li><strong>Click "Step":</strong> Watch the model predict one token. Observe the probability distribution.</li>
                <li><strong>Adjust Temperature:</strong> Slide the temperature control and step again. See how the distribution changes.</li>
                <li><strong>Repeat:</strong> Keep clicking "Step" to build a full response, token by token.</li>
            </ol>

            <p>You'll quickly develop an intuition for which types of prompts lead to confident predictions and which ones push the model into guessing territory.</p>

            <div class="cta-section">
                <p><strong>Ready to see how LLMs really work?</strong></p>
                <a href="autoregressive-playground.html" class="cta-button">Open the Playground →</a>
            </div>

            <hr>

            <h2>The Bottom Line</h2>

            <p>AI strategy in 2026 isn't about "prompting better." It's about understanding the constraints of the technology so you can architect systems that work around them.</p>

            <p>When you see a low-probability token selected, you don't just accept it—you redesign your prompt structure to avoid that uncertainty in the first place. When you need deterministic outputs, you lock temperature to 0 and validate that your system isn't sampling. When you need creativity, you crank temperature up and implement human review for outlier outputs.</p>

            <p>That's the difference between using AI and <em>deploying</em> AI at scale.</p>

            <p>The Playground is a teaching tool, but the principles apply to every AI project in your organization. Play with it. Break it. Watch the tokens fly. You'll walk away with a mental model that most executives don't have—and that asymmetry is your competitive advantage.</p>
        </div>
    </article>

    <div class="article-footer">
        <p>Written by <a href="index.html">Aditya V Jain</a> · Product Lead @ Google</p>
    </div>
</div>

</body>
</html>
